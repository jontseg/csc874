{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87aa0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e66edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews',with_info=True,as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74642f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_dataset.take(4000)\n",
    "test = test_dataset.take(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7880975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to shuffle the data...\n",
    "BUFFER_SIZE = 4000 # We will put all the data into this big buffer, and sample randomly from the buffer\n",
    "BATCH_SIZE = 128 # We will read 128 reviews at a time\n",
    "\n",
    "train = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65d8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.prefetch(BUFFER_SIZE)\n",
    "test = test.prefetch(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2654698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000 #assuming our vocabulary is just 1000 words\n",
    "\n",
    "encoder = layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train.map(lambda text, label: text))\n",
    "# we just  encode teh text not the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774dfff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'i', 'it',\n",
       "       'this', 'that', 'br', 'was', 'as', 'with', 'for', 'but', 'movie'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are the first 20 words in our 1000-word vocabulary\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e396463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example, label = list(train.take(1))[0] # that's one batch\n",
    "\n",
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34379b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"I've seen the original non-dubbed German version and I was surprised how bad this movie actually is. Thinking I had seen my share of bad movies like Ghoulies 2, Rabid Grannies, Zombie Lake and such, nothing could've prepared me for this! It really was a pain to sit through this flick, as there's no plot, no good acting and even the special effects aren't convincing, especially the so-called zombies, wearing nothing more than white make-up and their old clothes, so their good set wouldn't be ruined by ketchup and marmalade stains. <br /><br />If you really want to waste 90 minutes of your life, then watch it, for all the others, don't do it, because you WILL regret it!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334babc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211, 111,   2, 203,   1,   1, 299,   3,   9,  14, 715,  87,  88,\n",
       "         11,  19, 160,   7, 510,   9,  63, 111,  54,   1,   5,  88,  99,\n",
       "         39,   1, 267,   1,   1, 881,   1,   3, 139, 162,   1,   1,  70,\n",
       "         17,  11,  10,  62,  14,   4,   1,   6, 842, 135,  11, 556,  15,\n",
       "        222,  58, 106,  58,  50, 112,   3,  55,   2, 316, 309, 595,   1,\n",
       "        260,   2,   1,   1,   1, 162,  51,  71, 493,   1,   3,  66, 166,\n",
       "          1,  36,  66,  50, 280, 584,  28,   1,  33,   1,   3,   1,   1,\n",
       "         13,  13,  43,  25,  62, 177,   6, 462,   1, 232,   5, 133, 115,\n",
       "         91, 104,  10,  17,  32,   2, 402,  89,  82,  10,  81,  25,  83,\n",
       "          1,  10]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example[:1]).numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4a5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([encoder, #the encoder\n",
    "                             tf.keras.layers.Embedding(\n",
    "                             input_dim=len(encoder.get_vocabulary()),\n",
    "                             output_dim=64,\n",
    "                             # Use masking to handle the variable sequence lengths\n",
    "                             mask_zero=True),\n",
    "                             tf.keras.layers.Bidirectional(layers.LSTM(64)),\n",
    "                             # making LSTM Bidirectional\n",
    "                             tf.keras.layers.Dense(32, activation='relu'), \n",
    "                             # FC layer for the classification part\n",
    "                             tf.keras.layers.Dense(1) # final FC layer\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbda58c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00707749]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics were out of this world. I would reccomend this movie' )\n",
    "\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ae53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use binary cross entropy again because this isa  binary classification\n",
    "# problem because this is a binary classification task (positive or negative)\n",
    "# we also did not apply a sigmoid activation fucntion at the last FC layer,\n",
    "# so we specify that they are calculating the cross entropy from logits\n",
    "\n",
    "model.compile(\n",
    "\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#adam optimizer is more efficient (not always the most accurate though)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01c4c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 64)          64000     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,209\n",
      "Trainable params: 134,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39028ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "32/32 [==============================] - 361s 11s/step - loss: 0.6930 - accuracy: 0.4967 - val_loss: 0.6928 - val_accuracy: 0.5030\n",
      "Epoch 2/25\n",
      "32/32 [==============================] - 382s 12s/step - loss: 0.6919 - accuracy: 0.4967 - val_loss: 0.6922 - val_accuracy: 0.5030\n",
      "Epoch 3/25\n",
      "32/32 [==============================] - 433s 14s/step - loss: 0.6904 - accuracy: 0.4967 - val_loss: 0.6910 - val_accuracy: 0.5030\n",
      "Epoch 4/25\n",
      "32/32 [==============================] - 420s 13s/step - loss: 0.6875 - accuracy: 0.4967 - val_loss: 0.6881 - val_accuracy: 0.5030\n",
      "Epoch 5/25\n",
      "32/32 [==============================] - 454s 14s/step - loss: 0.6792 - accuracy: 0.4967 - val_loss: 0.6771 - val_accuracy: 0.5100\n",
      "Epoch 6/25\n",
      "32/32 [==============================] - 458s 14s/step - loss: 0.6281 - accuracy: 0.6075 - val_loss: 0.6137 - val_accuracy: 0.6380\n",
      "Epoch 7/25\n",
      "32/32 [==============================] - 451s 14s/step - loss: 0.5633 - accuracy: 0.7060 - val_loss: 0.5708 - val_accuracy: 0.7440\n",
      "Epoch 8/25\n",
      "32/32 [==============================] - 458s 14s/step - loss: 0.5336 - accuracy: 0.7585 - val_loss: 0.5807 - val_accuracy: 0.6740\n",
      "Epoch 9/25\n",
      "32/32 [==============================] - 457s 14s/step - loss: 0.5107 - accuracy: 0.7632 - val_loss: 0.5353 - val_accuracy: 0.7390\n",
      "Epoch 10/25\n",
      "32/32 [==============================] - 433s 13s/step - loss: 0.4817 - accuracy: 0.8070 - val_loss: 0.5305 - val_accuracy: 0.7630\n",
      "Epoch 11/25\n",
      "32/32 [==============================] - 459s 15s/step - loss: 0.4401 - accuracy: 0.8255 - val_loss: 0.4943 - val_accuracy: 0.7800\n",
      "Epoch 12/25\n",
      "32/32 [==============================] - 464s 15s/step - loss: 0.4095 - accuracy: 0.8317 - val_loss: 0.4814 - val_accuracy: 0.7980\n",
      "Epoch 13/25\n",
      "32/32 [==============================] - 458s 14s/step - loss: 0.3841 - accuracy: 0.8432 - val_loss: 0.4640 - val_accuracy: 0.8070\n",
      "Epoch 14/25\n",
      "32/32 [==============================] - 470s 14s/step - loss: 0.3652 - accuracy: 0.8510 - val_loss: 0.4663 - val_accuracy: 0.8030\n",
      "Epoch 15/25\n",
      "32/32 [==============================] - 458s 14s/step - loss: 0.3476 - accuracy: 0.8602 - val_loss: 0.4330 - val_accuracy: 0.8090\n",
      "Epoch 16/25\n",
      "32/32 [==============================] - 27906s 900s/step - loss: 0.3375 - accuracy: 0.8692 - val_loss: 0.4536 - val_accuracy: 0.7990\n",
      "Epoch 17/25\n",
      "32/32 [==============================] - 462s 15s/step - loss: 0.3309 - accuracy: 0.8700 - val_loss: 0.4645 - val_accuracy: 0.8010\n",
      "Epoch 18/25\n",
      "32/32 [==============================] - 461s 14s/step - loss: 0.3079 - accuracy: 0.8780 - val_loss: 0.4468 - val_accuracy: 0.8050\n",
      "Epoch 19/25\n",
      "32/32 [==============================] - 468s 15s/step - loss: 0.3029 - accuracy: 0.8800 - val_loss: 0.4424 - val_accuracy: 0.8170\n",
      "Epoch 20/25\n",
      "32/32 [==============================] - 459s 12s/step - loss: 0.2985 - accuracy: 0.8820 - val_loss: 0.4363 - val_accuracy: 0.8090\n",
      "Epoch 21/25\n",
      "32/32 [==============================] - 464s 15s/step - loss: 0.2874 - accuracy: 0.8865 - val_loss: 0.4264 - val_accuracy: 0.8130\n",
      "Epoch 22/25\n",
      "32/32 [==============================] - 456s 14s/step - loss: 0.2789 - accuracy: 0.8913 - val_loss: 0.4525 - val_accuracy: 0.8050\n",
      "Epoch 23/25\n",
      "32/32 [==============================] - 471s 15s/step - loss: 0.2748 - accuracy: 0.8947 - val_loss: 0.4243 - val_accuracy: 0.8180\n",
      "Epoch 24/25\n",
      "32/32 [==============================] - 470s 15s/step - loss: 0.2730 - accuracy: 0.8963 - val_loss: 0.4183 - val_accuracy: 0.8240\n",
      "Epoch 25/25\n",
      "32/32 [==============================] - 486s 15s/step - loss: 0.2641 - accuracy: 0.8988 - val_loss: 0.4254 - val_accuracy: 0.8130\n"
     ]
    }
   ],
   "source": [
    "H2 = model.fit(train, epochs=25, validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc11918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
